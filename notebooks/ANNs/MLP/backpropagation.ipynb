{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6d01d8",
   "metadata": {},
   "source": [
    "# **BACKPROPAGATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d53428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4],[7,9,5],[6,10,6],[5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4831ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>profile_score</th>\n",
       "      <th>lpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa  profile_score  lpa\n",
       "0     8              8    4\n",
       "1     7              9    5\n",
       "2     6             10    6\n",
       "3     5             12    7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d83f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "  \n",
    "  np.random.seed(3)\n",
    "  parameters = {}\n",
    "  L = len(layer_dims)         \n",
    "\n",
    "  for l in range(1, L):\n",
    "\n",
    "    parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l]))*0.1\n",
    "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "      \n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d91cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "  \n",
    "  Z = np.dot(W.T, A_prev) + b\n",
    "  \n",
    "  return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e226aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Prop\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "  A = X\n",
    "  L = len(parameters) // 2                  # number of layers in the neural network\n",
    "  \n",
    "  for l in range(1, L+1):\n",
    "    A_prev = A \n",
    "    Wl = parameters['W' + str(l)]\n",
    "    bl = parameters['b' + str(l)]\n",
    "    #print(\"A\"+str(l-1)+\": \", A_prev)\n",
    "    #print(\"W\"+str(l)+\": \", Wl)\n",
    "    #print(\"b\"+str(l)+\": \", bl)\n",
    "    #print(\"--\"*20)\n",
    "\n",
    "    A = linear_forward(A_prev, Wl, bl)\n",
    "    #print(\"A\"+str(l)+\": \", A)\n",
    "    #print(\"**\"*20)\n",
    "          \n",
    "  return A,A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c890e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[0][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34c2a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066102c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6],\n",
       "       [1.6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e6f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,y,y_hat,A1,X):\n",
    "  parameters['W2'][0][0] = parameters['W2'][0][0] + (0.001 * 2 * (y - y_hat)*A1[0][0])\n",
    "  parameters['W2'][1][0] = parameters['W2'][1][0] + (0.001 * 2 * (y - y_hat)*A1[1][0])\n",
    "  parameters['b2'][0][0] = parameters['W2'][1][0] + (0.001 * 2 * (y - y_hat))\n",
    "\n",
    "  parameters['W1'][0][0] = parameters['W1'][0][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0]*X[0][0])\n",
    "  parameters['W1'][0][1] = parameters['W1'][0][1] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0]*X[1][0])\n",
    "  parameters['b1'][0][0] = parameters['b1'][0][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0])\n",
    "\n",
    "  parameters['W1'][1][0] = parameters['W1'][1][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0]*X[0][0])\n",
    "  parameters['W1'][1][1] = parameters['W1'][1][1] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0]*X[1][0])\n",
    "  parameters['b1'][1][0] = parameters['b1'][1][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdde20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10658137, 0.10658137],\n",
       "        [0.10658137, 0.10658137]]),\n",
       " 'b1': array([[0.00082267],\n",
       "        [0.00082267]]),\n",
       " 'W2': array([[0.111776],\n",
       "        [0.111776]]),\n",
       " 'b2': array([[0.119136]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[0][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cc8cccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10753323, 0.10968558],\n",
       "        [0.10753323, 0.10968558]]),\n",
       " 'b1': array([[0.00107618],\n",
       "        [0.00107618]]),\n",
       " 'W2': array([[0.114976],\n",
       "        [0.114976]]),\n",
       " 'b2': array([[0.124336]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[1].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[1][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd7b150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10805488, 0.11342479],\n",
       "        [0.10805488, 0.11342479]]),\n",
       " 'b1': array([[0.00134248],\n",
       "        [0.00134248]]),\n",
       " 'W2': array([[0.118176],\n",
       "        [0.118176]]),\n",
       " 'b2': array([[0.129536]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[2].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[2][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bc8786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  25.321744156025517\n",
      "Epoch -  2 Loss -  18.320004165722047\n",
      "Epoch -  3 Loss -  9.473661050729628\n",
      "Epoch -  4 Loss -  3.2520938634031613\n",
      "Epoch -  5 Loss -  1.3407132589299962\n",
      "Epoch -  6 Loss -  1.1726178458115697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.26628395, 0.3978842 ],\n",
       "        [0.27952594, 0.4243817 ]]),\n",
       " 'b1': array([[0.02826156],\n",
       "        [0.03066415]]),\n",
       " 'W2': array([[0.41973268],\n",
       "        [0.49530113]]),\n",
       " 'b2': array([[0.49808206]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "epochs = 6\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(df.shape[0]):\n",
    "\n",
    "    X = df[['cgpa', 'profile_score']].values[j].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "    y = df[['lpa']].values[j][0]\n",
    "\n",
    "    # Parameter initialization\n",
    "\n",
    "\n",
    "    y_hat,A1 = L_layer_forward(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5555b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "df = pd.DataFrame([[8,8,4],\n",
    "                   [7,9,5],\n",
    "                   [6,10,6],\n",
    "                   [5,12,7]], \n",
    "                   columns=['cgpa', 'profile_score', 'lpa'])\n",
    "\n",
    "# ---------- Initialize Parameters ----------\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters\n",
    "\n",
    "# ---------- Forward Propagation ----------\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    return Z\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z = linear_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)])\n",
    "        A = sigmoid(Z)\n",
    "        caches.append((A_prev, parameters['W'+str(l)], parameters['b'+str(l)], Z))\n",
    "\n",
    "    # output layer (no activation or use sigmoid)\n",
    "    ZL = linear_forward(A, parameters['W'+str(L)], parameters['b'+str(L)])\n",
    "    AL = ZL  # linear output for regression\n",
    "    caches.append((A, parameters['W'+str(L)], parameters['b'+str(L)], ZL))\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "# ---------- Backward Propagation ----------\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b, Z = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    s = sigmoid(Z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def L_layer_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    # dAL = -2 * (Y - AL)  # MSE loss derivative\n",
    "    dAL = -2 * (Y - AL)   # d(Loss)/dAL\n",
    "\n",
    "    # Output layer\n",
    "    current_cache = caches[L-1]\n",
    "    dZL = dAL  # since output layer is linear\n",
    "    dA_prev, dW, db = linear_backward(dZL, current_cache)\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "\n",
    "    # Hidden layers\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dZ = grads[\"dA\" + str(l+1)] * sigmoid_derivative(current_cache[3])\n",
    "        dA_prev, dW, db = linear_backward(dZ, current_cache)\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "\n",
    "    return grads\n",
    "\n",
    "# ---------- Parameter Update ----------\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        parameters['W' + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters\n",
    "\n",
    "# ---------- Training ----------\n",
    "def train_model(X, Y, layer_dims, learning_rate=0.01, epochs=5000):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    loss_list = []\n",
    "    for i in range(epochs):\n",
    "        AL, caches = L_layer_forward(X, parameters)\n",
    "        loss = np.mean((Y - AL)**2)\n",
    "\n",
    "        grads = L_layer_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
    "        loss_list.append(loss)\n",
    "    return parameters,loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb245c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 30.1076\n",
      "Epoch 1, Loss: 28.4072\n",
      "Epoch 2, Loss: 26.7986\n",
      "Epoch 3, Loss: 25.2751\n",
      "Epoch 4, Loss: 23.8307\n",
      "Epoch 5, Loss: 22.4603\n",
      "Epoch 6, Loss: 21.1595\n",
      "Epoch 7, Loss: 19.9241\n",
      "Epoch 8, Loss: 18.7506\n",
      "Epoch 9, Loss: 17.6358\n",
      "Epoch 10, Loss: 16.5766\n",
      "Epoch 11, Loss: 15.5705\n",
      "Epoch 12, Loss: 14.6149\n",
      "Epoch 13, Loss: 13.7076\n",
      "Epoch 14, Loss: 12.8464\n",
      "Epoch 15, Loss: 12.0295\n",
      "Epoch 16, Loss: 11.2552\n",
      "Epoch 17, Loss: 10.5220\n",
      "Epoch 18, Loss: 9.8285\n",
      "Epoch 19, Loss: 9.1737\n",
      "Epoch 20, Loss: 8.5563\n",
      "Epoch 21, Loss: 7.9755\n",
      "Epoch 22, Loss: 7.4304\n",
      "Epoch 23, Loss: 6.9199\n",
      "Epoch 24, Loss: 6.4432\n",
      "Epoch 25, Loss: 5.9991\n",
      "Epoch 26, Loss: 5.5865\n",
      "Epoch 27, Loss: 5.2040\n",
      "Epoch 28, Loss: 4.8504\n",
      "Epoch 29, Loss: 4.5241\n",
      "Epoch 30, Loss: 4.2239\n",
      "Epoch 31, Loss: 3.9480\n",
      "Epoch 32, Loss: 3.6951\n",
      "Epoch 33, Loss: 3.4637\n",
      "Epoch 34, Loss: 3.2523\n",
      "Epoch 35, Loss: 3.0594\n",
      "Epoch 36, Loss: 2.8837\n",
      "Epoch 37, Loss: 2.7240\n",
      "Epoch 38, Loss: 2.5788\n",
      "Epoch 39, Loss: 2.4471\n",
      "Epoch 40, Loss: 2.3277\n",
      "Epoch 41, Loss: 2.2196\n",
      "Epoch 42, Loss: 2.1219\n",
      "Epoch 43, Loss: 2.0335\n",
      "Epoch 44, Loss: 1.9537\n",
      "Epoch 45, Loss: 1.8817\n",
      "Epoch 46, Loss: 1.8167\n",
      "Epoch 47, Loss: 1.7582\n",
      "Epoch 48, Loss: 1.7055\n",
      "Epoch 49, Loss: 1.6581\n",
      "Epoch 50, Loss: 1.6154\n",
      "Epoch 51, Loss: 1.5770\n",
      "Epoch 52, Loss: 1.5425\n",
      "Epoch 53, Loss: 1.5115\n",
      "Epoch 54, Loss: 1.4837\n",
      "Epoch 55, Loss: 1.4587\n",
      "Epoch 56, Loss: 1.4362\n",
      "Epoch 57, Loss: 1.4161\n",
      "Epoch 58, Loss: 1.3980\n",
      "Epoch 59, Loss: 1.3818\n",
      "Epoch 60, Loss: 1.3673\n",
      "Epoch 61, Loss: 1.3543\n",
      "Epoch 62, Loss: 1.3426\n",
      "Epoch 63, Loss: 1.3322\n",
      "Epoch 64, Loss: 1.3228\n",
      "Epoch 65, Loss: 1.3144\n",
      "Epoch 66, Loss: 1.3069\n",
      "Epoch 67, Loss: 1.3001\n",
      "Epoch 68, Loss: 1.2941\n",
      "Epoch 69, Loss: 1.2887\n",
      "Epoch 70, Loss: 1.2838\n",
      "Epoch 71, Loss: 1.2795\n",
      "Epoch 72, Loss: 1.2756\n",
      "Epoch 73, Loss: 1.2721\n",
      "Epoch 74, Loss: 1.2690\n",
      "Epoch 75, Loss: 1.2662\n",
      "Epoch 76, Loss: 1.2637\n",
      "Epoch 77, Loss: 1.2615\n",
      "Epoch 78, Loss: 1.2595\n",
      "Epoch 79, Loss: 1.2577\n",
      "Epoch 80, Loss: 1.2560\n",
      "Epoch 81, Loss: 1.2546\n",
      "Epoch 82, Loss: 1.2533\n",
      "Epoch 83, Loss: 1.2521\n",
      "Epoch 84, Loss: 1.2511\n",
      "Epoch 85, Loss: 1.2501\n",
      "Epoch 86, Loss: 1.2493\n",
      "Epoch 87, Loss: 1.2485\n",
      "Epoch 88, Loss: 1.2478\n",
      "Epoch 89, Loss: 1.2472\n",
      "Epoch 90, Loss: 1.2466\n",
      "Epoch 91, Loss: 1.2461\n",
      "Epoch 92, Loss: 1.2456\n",
      "Epoch 93, Loss: 1.2452\n",
      "Epoch 94, Loss: 1.2448\n",
      "Epoch 95, Loss: 1.2444\n",
      "Epoch 96, Loss: 1.2441\n",
      "Epoch 97, Loss: 1.2438\n",
      "Epoch 98, Loss: 1.2436\n",
      "Epoch 99, Loss: 1.2433\n",
      "Epoch 100, Loss: 1.2431\n",
      "Epoch 101, Loss: 1.2429\n",
      "Epoch 102, Loss: 1.2427\n",
      "Epoch 103, Loss: 1.2425\n",
      "Epoch 104, Loss: 1.2423\n",
      "Epoch 105, Loss: 1.2421\n",
      "Epoch 106, Loss: 1.2420\n",
      "Epoch 107, Loss: 1.2418\n",
      "Epoch 108, Loss: 1.2417\n",
      "Epoch 109, Loss: 1.2415\n",
      "Epoch 110, Loss: 1.2414\n",
      "Epoch 111, Loss: 1.2413\n",
      "Epoch 112, Loss: 1.2412\n",
      "Epoch 113, Loss: 1.2411\n",
      "Epoch 114, Loss: 1.2409\n",
      "Epoch 115, Loss: 1.2408\n",
      "Epoch 116, Loss: 1.2407\n",
      "Epoch 117, Loss: 1.2406\n",
      "Epoch 118, Loss: 1.2405\n",
      "Epoch 119, Loss: 1.2404\n",
      "Epoch 120, Loss: 1.2403\n",
      "Epoch 121, Loss: 1.2402\n",
      "Epoch 122, Loss: 1.2401\n",
      "Epoch 123, Loss: 1.2400\n",
      "Epoch 124, Loss: 1.2399\n",
      "Epoch 125, Loss: 1.2398\n",
      "Epoch 126, Loss: 1.2397\n",
      "Epoch 127, Loss: 1.2396\n",
      "Epoch 128, Loss: 1.2395\n",
      "Epoch 129, Loss: 1.2394\n",
      "Epoch 130, Loss: 1.2392\n",
      "Epoch 131, Loss: 1.2391\n",
      "Epoch 132, Loss: 1.2390\n",
      "Epoch 133, Loss: 1.2389\n",
      "Epoch 134, Loss: 1.2388\n",
      "Epoch 135, Loss: 1.2387\n",
      "Epoch 136, Loss: 1.2386\n",
      "Epoch 137, Loss: 1.2385\n",
      "Epoch 138, Loss: 1.2384\n",
      "Epoch 139, Loss: 1.2382\n",
      "Epoch 140, Loss: 1.2381\n",
      "Epoch 141, Loss: 1.2380\n",
      "Epoch 142, Loss: 1.2379\n",
      "Epoch 143, Loss: 1.2378\n",
      "Epoch 144, Loss: 1.2376\n",
      "Epoch 145, Loss: 1.2375\n",
      "Epoch 146, Loss: 1.2374\n",
      "Epoch 147, Loss: 1.2372\n",
      "Epoch 148, Loss: 1.2371\n",
      "Epoch 149, Loss: 1.2370\n",
      "Epoch 150, Loss: 1.2368\n",
      "Epoch 151, Loss: 1.2367\n",
      "Epoch 152, Loss: 1.2365\n",
      "Epoch 153, Loss: 1.2364\n",
      "Epoch 154, Loss: 1.2363\n",
      "Epoch 155, Loss: 1.2361\n",
      "Epoch 156, Loss: 1.2360\n",
      "Epoch 157, Loss: 1.2358\n",
      "Epoch 158, Loss: 1.2356\n",
      "Epoch 159, Loss: 1.2355\n",
      "Epoch 160, Loss: 1.2353\n",
      "Epoch 161, Loss: 1.2351\n",
      "Epoch 162, Loss: 1.2350\n",
      "Epoch 163, Loss: 1.2348\n",
      "Epoch 164, Loss: 1.2346\n",
      "Epoch 165, Loss: 1.2344\n",
      "Epoch 166, Loss: 1.2343\n",
      "Epoch 167, Loss: 1.2341\n",
      "Epoch 168, Loss: 1.2339\n",
      "Epoch 169, Loss: 1.2337\n",
      "Epoch 170, Loss: 1.2335\n",
      "Epoch 171, Loss: 1.2333\n",
      "Epoch 172, Loss: 1.2331\n",
      "Epoch 173, Loss: 1.2329\n",
      "Epoch 174, Loss: 1.2326\n",
      "Epoch 175, Loss: 1.2324\n",
      "Epoch 176, Loss: 1.2322\n",
      "Epoch 177, Loss: 1.2320\n",
      "Epoch 178, Loss: 1.2317\n",
      "Epoch 179, Loss: 1.2315\n",
      "Epoch 180, Loss: 1.2313\n",
      "Epoch 181, Loss: 1.2310\n",
      "Epoch 182, Loss: 1.2308\n",
      "Epoch 183, Loss: 1.2305\n",
      "Epoch 184, Loss: 1.2302\n",
      "Epoch 185, Loss: 1.2300\n",
      "Epoch 186, Loss: 1.2297\n",
      "Epoch 187, Loss: 1.2294\n",
      "Epoch 188, Loss: 1.2291\n",
      "Epoch 189, Loss: 1.2288\n",
      "Epoch 190, Loss: 1.2285\n",
      "Epoch 191, Loss: 1.2282\n",
      "Epoch 192, Loss: 1.2279\n",
      "Epoch 193, Loss: 1.2276\n",
      "Epoch 194, Loss: 1.2272\n",
      "Epoch 195, Loss: 1.2269\n",
      "Epoch 196, Loss: 1.2266\n",
      "Epoch 197, Loss: 1.2262\n",
      "Epoch 198, Loss: 1.2259\n",
      "Epoch 199, Loss: 1.2255\n",
      "Epoch 200, Loss: 1.2251\n",
      "Epoch 201, Loss: 1.2247\n",
      "Epoch 202, Loss: 1.2243\n",
      "Epoch 203, Loss: 1.2239\n",
      "Epoch 204, Loss: 1.2235\n",
      "Epoch 205, Loss: 1.2231\n",
      "Epoch 206, Loss: 1.2227\n",
      "Epoch 207, Loss: 1.2222\n",
      "Epoch 208, Loss: 1.2218\n",
      "Epoch 209, Loss: 1.2213\n",
      "Epoch 210, Loss: 1.2209\n",
      "Epoch 211, Loss: 1.2204\n",
      "Epoch 212, Loss: 1.2199\n",
      "Epoch 213, Loss: 1.2194\n",
      "Epoch 214, Loss: 1.2189\n",
      "Epoch 215, Loss: 1.2184\n",
      "Epoch 216, Loss: 1.2179\n",
      "Epoch 217, Loss: 1.2173\n",
      "Epoch 218, Loss: 1.2168\n",
      "Epoch 219, Loss: 1.2162\n",
      "Epoch 220, Loss: 1.2157\n",
      "Epoch 221, Loss: 1.2151\n",
      "Epoch 222, Loss: 1.2145\n",
      "Epoch 223, Loss: 1.2139\n",
      "Epoch 224, Loss: 1.2133\n",
      "Epoch 225, Loss: 1.2126\n",
      "Epoch 226, Loss: 1.2120\n",
      "Epoch 227, Loss: 1.2113\n",
      "Epoch 228, Loss: 1.2107\n",
      "Epoch 229, Loss: 1.2100\n",
      "Epoch 230, Loss: 1.2093\n",
      "Epoch 231, Loss: 1.2086\n",
      "Epoch 232, Loss: 1.2079\n",
      "Epoch 233, Loss: 1.2072\n",
      "Epoch 234, Loss: 1.2065\n",
      "Epoch 235, Loss: 1.2058\n",
      "Epoch 236, Loss: 1.2051\n",
      "Epoch 237, Loss: 1.2043\n",
      "Epoch 238, Loss: 1.2035\n",
      "Epoch 239, Loss: 1.2028\n",
      "Epoch 240, Loss: 1.2020\n",
      "Epoch 241, Loss: 1.2012\n",
      "Epoch 242, Loss: 1.2004\n",
      "Epoch 243, Loss: 1.1996\n",
      "Epoch 244, Loss: 1.1988\n",
      "Epoch 245, Loss: 1.1980\n",
      "Epoch 246, Loss: 1.1972\n",
      "Epoch 247, Loss: 1.1964\n",
      "Epoch 248, Loss: 1.1956\n",
      "Epoch 249, Loss: 1.1947\n",
      "Epoch 250, Loss: 1.1939\n",
      "Epoch 251, Loss: 1.1930\n",
      "Epoch 252, Loss: 1.1922\n",
      "Epoch 253, Loss: 1.1914\n",
      "Epoch 254, Loss: 1.1905\n",
      "Epoch 255, Loss: 1.1896\n",
      "Epoch 256, Loss: 1.1888\n",
      "Epoch 257, Loss: 1.1879\n",
      "Epoch 258, Loss: 1.1871\n",
      "Epoch 259, Loss: 1.1862\n",
      "Epoch 260, Loss: 1.1853\n",
      "Epoch 261, Loss: 1.1844\n",
      "Epoch 262, Loss: 1.1836\n",
      "Epoch 263, Loss: 1.1827\n",
      "Epoch 264, Loss: 1.1818\n",
      "Epoch 265, Loss: 1.1809\n",
      "Epoch 266, Loss: 1.1801\n",
      "Epoch 267, Loss: 1.1792\n",
      "Epoch 268, Loss: 1.1783\n",
      "Epoch 269, Loss: 1.1774\n",
      "Epoch 270, Loss: 1.1765\n",
      "Epoch 271, Loss: 1.1757\n",
      "Epoch 272, Loss: 1.1748\n",
      "Epoch 273, Loss: 1.1739\n",
      "Epoch 274, Loss: 1.1730\n",
      "Epoch 275, Loss: 1.1721\n",
      "Epoch 276, Loss: 1.1712\n",
      "Epoch 277, Loss: 1.1704\n",
      "Epoch 278, Loss: 1.1695\n",
      "Epoch 279, Loss: 1.1686\n",
      "Epoch 280, Loss: 1.1677\n",
      "Epoch 281, Loss: 1.1668\n",
      "Epoch 282, Loss: 1.1659\n",
      "Epoch 283, Loss: 1.1650\n",
      "Epoch 284, Loss: 1.1642\n",
      "Epoch 285, Loss: 1.1633\n",
      "Epoch 286, Loss: 1.1624\n",
      "Epoch 287, Loss: 1.1615\n",
      "Epoch 288, Loss: 1.1606\n",
      "Epoch 289, Loss: 1.1597\n",
      "Epoch 290, Loss: 1.1588\n",
      "Epoch 291, Loss: 1.1579\n",
      "Epoch 292, Loss: 1.1570\n",
      "Epoch 293, Loss: 1.1561\n",
      "Epoch 294, Loss: 1.1552\n",
      "Epoch 295, Loss: 1.1543\n",
      "Epoch 296, Loss: 1.1534\n",
      "Epoch 297, Loss: 1.1525\n",
      "Epoch 298, Loss: 1.1516\n",
      "Epoch 299, Loss: 1.1507\n",
      "Epoch 300, Loss: 1.1498\n",
      "Epoch 301, Loss: 1.1489\n",
      "Epoch 302, Loss: 1.1480\n",
      "Epoch 303, Loss: 1.1471\n",
      "Epoch 304, Loss: 1.1461\n",
      "Epoch 305, Loss: 1.1452\n",
      "Epoch 306, Loss: 1.1443\n",
      "Epoch 307, Loss: 1.1434\n",
      "Epoch 308, Loss: 1.1425\n",
      "Epoch 309, Loss: 1.1415\n",
      "Epoch 310, Loss: 1.1406\n",
      "Epoch 311, Loss: 1.1397\n",
      "Epoch 312, Loss: 1.1387\n",
      "Epoch 313, Loss: 1.1378\n",
      "Epoch 314, Loss: 1.1369\n",
      "Epoch 315, Loss: 1.1359\n",
      "Epoch 316, Loss: 1.1350\n",
      "Epoch 317, Loss: 1.1340\n",
      "Epoch 318, Loss: 1.1331\n",
      "Epoch 319, Loss: 1.1322\n",
      "Epoch 320, Loss: 1.1312\n",
      "Epoch 321, Loss: 1.1302\n",
      "Epoch 322, Loss: 1.1293\n",
      "Epoch 323, Loss: 1.1283\n",
      "Epoch 324, Loss: 1.1274\n",
      "Epoch 325, Loss: 1.1264\n",
      "Epoch 326, Loss: 1.1254\n",
      "Epoch 327, Loss: 1.1245\n",
      "Epoch 328, Loss: 1.1235\n",
      "Epoch 329, Loss: 1.1225\n",
      "Epoch 330, Loss: 1.1215\n",
      "Epoch 331, Loss: 1.1205\n",
      "Epoch 332, Loss: 1.1196\n",
      "Epoch 333, Loss: 1.1186\n",
      "Epoch 334, Loss: 1.1176\n",
      "Epoch 335, Loss: 1.1166\n",
      "Epoch 336, Loss: 1.1156\n",
      "Epoch 337, Loss: 1.1146\n",
      "Epoch 338, Loss: 1.1136\n",
      "Epoch 339, Loss: 1.1126\n",
      "Epoch 340, Loss: 1.1115\n",
      "Epoch 341, Loss: 1.1105\n",
      "Epoch 342, Loss: 1.1095\n",
      "Epoch 343, Loss: 1.1085\n",
      "Epoch 344, Loss: 1.1074\n",
      "Epoch 345, Loss: 1.1064\n",
      "Epoch 346, Loss: 1.1054\n",
      "Epoch 347, Loss: 1.1043\n",
      "Epoch 348, Loss: 1.1033\n",
      "Epoch 349, Loss: 1.1023\n",
      "Epoch 350, Loss: 1.1012\n",
      "Epoch 351, Loss: 1.1002\n",
      "Epoch 352, Loss: 1.0991\n",
      "Epoch 353, Loss: 1.0980\n",
      "Epoch 354, Loss: 1.0970\n",
      "Epoch 355, Loss: 1.0959\n",
      "Epoch 356, Loss: 1.0948\n",
      "Epoch 357, Loss: 1.0938\n",
      "Epoch 358, Loss: 1.0927\n",
      "Epoch 359, Loss: 1.0916\n",
      "Epoch 360, Loss: 1.0905\n",
      "Epoch 361, Loss: 1.0894\n",
      "Epoch 362, Loss: 1.0883\n",
      "Epoch 363, Loss: 1.0872\n",
      "Epoch 364, Loss: 1.0861\n",
      "Epoch 365, Loss: 1.0850\n",
      "Epoch 366, Loss: 1.0839\n",
      "Epoch 367, Loss: 1.0828\n",
      "Epoch 368, Loss: 1.0816\n",
      "Epoch 369, Loss: 1.0805\n",
      "Epoch 370, Loss: 1.0794\n",
      "Epoch 371, Loss: 1.0782\n",
      "Epoch 372, Loss: 1.0771\n",
      "Epoch 373, Loss: 1.0759\n",
      "Epoch 374, Loss: 1.0748\n",
      "Epoch 375, Loss: 1.0736\n",
      "Epoch 376, Loss: 1.0725\n",
      "Epoch 377, Loss: 1.0713\n",
      "Epoch 378, Loss: 1.0701\n",
      "Epoch 379, Loss: 1.0690\n",
      "Epoch 380, Loss: 1.0678\n",
      "Epoch 381, Loss: 1.0666\n",
      "Epoch 382, Loss: 1.0654\n",
      "Epoch 383, Loss: 1.0642\n",
      "Epoch 384, Loss: 1.0630\n",
      "Epoch 385, Loss: 1.0618\n",
      "Epoch 386, Loss: 1.0606\n",
      "Epoch 387, Loss: 1.0594\n",
      "Epoch 388, Loss: 1.0582\n",
      "Epoch 389, Loss: 1.0569\n",
      "Epoch 390, Loss: 1.0557\n",
      "Epoch 391, Loss: 1.0545\n",
      "Epoch 392, Loss: 1.0532\n",
      "Epoch 393, Loss: 1.0520\n",
      "Epoch 394, Loss: 1.0507\n",
      "Epoch 395, Loss: 1.0495\n",
      "Epoch 396, Loss: 1.0482\n",
      "Epoch 397, Loss: 1.0469\n",
      "Epoch 398, Loss: 1.0457\n",
      "Epoch 399, Loss: 1.0444\n",
      "Epoch 400, Loss: 1.0431\n",
      "Epoch 401, Loss: 1.0418\n",
      "Epoch 402, Loss: 1.0405\n",
      "Epoch 403, Loss: 1.0392\n",
      "Epoch 404, Loss: 1.0379\n",
      "Epoch 405, Loss: 1.0366\n",
      "Epoch 406, Loss: 1.0352\n",
      "Epoch 407, Loss: 1.0339\n",
      "Epoch 408, Loss: 1.0326\n",
      "Epoch 409, Loss: 1.0312\n",
      "Epoch 410, Loss: 1.0299\n",
      "Epoch 411, Loss: 1.0285\n",
      "Epoch 412, Loss: 1.0272\n",
      "Epoch 413, Loss: 1.0258\n",
      "Epoch 414, Loss: 1.0244\n",
      "Epoch 415, Loss: 1.0230\n",
      "Epoch 416, Loss: 1.0216\n",
      "Epoch 417, Loss: 1.0202\n",
      "Epoch 418, Loss: 1.0188\n",
      "Epoch 419, Loss: 1.0174\n",
      "Epoch 420, Loss: 1.0160\n",
      "Epoch 421, Loss: 1.0146\n",
      "Epoch 422, Loss: 1.0132\n",
      "Epoch 423, Loss: 1.0117\n",
      "Epoch 424, Loss: 1.0103\n",
      "Epoch 425, Loss: 1.0089\n",
      "Epoch 426, Loss: 1.0074\n",
      "Epoch 427, Loss: 1.0059\n",
      "Epoch 428, Loss: 1.0045\n",
      "Epoch 429, Loss: 1.0030\n",
      "Epoch 430, Loss: 1.0015\n",
      "Epoch 431, Loss: 1.0000\n",
      "Epoch 432, Loss: 0.9985\n",
      "Epoch 433, Loss: 0.9970\n",
      "Epoch 434, Loss: 0.9955\n",
      "Epoch 435, Loss: 0.9940\n",
      "Epoch 436, Loss: 0.9925\n",
      "Epoch 437, Loss: 0.9910\n",
      "Epoch 438, Loss: 0.9894\n",
      "Epoch 439, Loss: 0.9879\n",
      "Epoch 440, Loss: 0.9863\n",
      "Epoch 441, Loss: 0.9848\n",
      "Epoch 442, Loss: 0.9832\n",
      "Epoch 443, Loss: 0.9817\n",
      "Epoch 444, Loss: 0.9801\n",
      "Epoch 445, Loss: 0.9785\n",
      "Epoch 446, Loss: 0.9769\n",
      "Epoch 447, Loss: 0.9753\n",
      "Epoch 448, Loss: 0.9737\n",
      "Epoch 449, Loss: 0.9721\n",
      "Epoch 450, Loss: 0.9705\n",
      "Epoch 451, Loss: 0.9689\n",
      "Epoch 452, Loss: 0.9672\n",
      "Epoch 453, Loss: 0.9656\n",
      "Epoch 454, Loss: 0.9639\n",
      "Epoch 455, Loss: 0.9623\n",
      "Epoch 456, Loss: 0.9606\n",
      "Epoch 457, Loss: 0.9590\n",
      "Epoch 458, Loss: 0.9573\n",
      "Epoch 459, Loss: 0.9556\n",
      "Epoch 460, Loss: 0.9539\n",
      "Epoch 461, Loss: 0.9522\n",
      "Epoch 462, Loss: 0.9505\n",
      "Epoch 463, Loss: 0.9488\n",
      "Epoch 464, Loss: 0.9471\n",
      "Epoch 465, Loss: 0.9454\n",
      "Epoch 466, Loss: 0.9437\n",
      "Epoch 467, Loss: 0.9420\n",
      "Epoch 468, Loss: 0.9402\n",
      "Epoch 469, Loss: 0.9385\n",
      "Epoch 470, Loss: 0.9367\n",
      "Epoch 471, Loss: 0.9350\n",
      "Epoch 472, Loss: 0.9332\n",
      "Epoch 473, Loss: 0.9314\n",
      "Epoch 474, Loss: 0.9296\n",
      "Epoch 475, Loss: 0.9279\n",
      "Epoch 476, Loss: 0.9261\n",
      "Epoch 477, Loss: 0.9243\n",
      "Epoch 478, Loss: 0.9225\n",
      "Epoch 479, Loss: 0.9207\n",
      "Epoch 480, Loss: 0.9188\n",
      "Epoch 481, Loss: 0.9170\n",
      "Epoch 482, Loss: 0.9152\n",
      "Epoch 483, Loss: 0.9134\n",
      "Epoch 484, Loss: 0.9115\n",
      "Epoch 485, Loss: 0.9097\n",
      "Epoch 486, Loss: 0.9078\n",
      "Epoch 487, Loss: 0.9059\n",
      "Epoch 488, Loss: 0.9041\n",
      "Epoch 489, Loss: 0.9022\n",
      "Epoch 490, Loss: 0.9003\n",
      "Epoch 491, Loss: 0.8984\n",
      "Epoch 492, Loss: 0.8965\n",
      "Epoch 493, Loss: 0.8946\n",
      "Epoch 494, Loss: 0.8927\n",
      "Epoch 495, Loss: 0.8908\n",
      "Epoch 496, Loss: 0.8889\n",
      "Epoch 497, Loss: 0.8869\n",
      "Epoch 498, Loss: 0.8850\n",
      "Epoch 499, Loss: 0.8831\n",
      "Epoch 500, Loss: 0.8811\n",
      "Epoch 501, Loss: 0.8792\n",
      "Epoch 502, Loss: 0.8772\n",
      "Epoch 503, Loss: 0.8752\n",
      "Epoch 504, Loss: 0.8732\n",
      "Epoch 505, Loss: 0.8712\n",
      "Epoch 506, Loss: 0.8693\n",
      "Epoch 507, Loss: 0.8673\n",
      "Epoch 508, Loss: 0.8652\n",
      "Epoch 509, Loss: 0.8632\n",
      "Epoch 510, Loss: 0.8612\n",
      "Epoch 511, Loss: 0.8592\n",
      "Epoch 512, Loss: 0.8571\n",
      "Epoch 513, Loss: 0.8551\n",
      "Epoch 514, Loss: 0.8530\n",
      "Epoch 515, Loss: 0.8510\n",
      "Epoch 516, Loss: 0.8489\n",
      "Epoch 517, Loss: 0.8468\n",
      "Epoch 518, Loss: 0.8447\n",
      "Epoch 519, Loss: 0.8426\n",
      "Epoch 520, Loss: 0.8405\n",
      "Epoch 521, Loss: 0.8384\n",
      "Epoch 522, Loss: 0.8363\n",
      "Epoch 523, Loss: 0.8342\n",
      "Epoch 524, Loss: 0.8321\n",
      "Epoch 525, Loss: 0.8299\n",
      "Epoch 526, Loss: 0.8278\n",
      "Epoch 527, Loss: 0.8256\n",
      "Epoch 528, Loss: 0.8235\n",
      "Epoch 529, Loss: 0.8213\n",
      "Epoch 530, Loss: 0.8191\n",
      "Epoch 531, Loss: 0.8169\n",
      "Epoch 532, Loss: 0.8147\n",
      "Epoch 533, Loss: 0.8125\n",
      "Epoch 534, Loss: 0.8103\n",
      "Epoch 535, Loss: 0.8081\n",
      "Epoch 536, Loss: 0.8058\n",
      "Epoch 537, Loss: 0.8036\n",
      "Epoch 538, Loss: 0.8013\n",
      "Epoch 539, Loss: 0.7991\n",
      "Epoch 540, Loss: 0.7968\n",
      "Epoch 541, Loss: 0.7945\n",
      "Epoch 542, Loss: 0.7922\n",
      "Epoch 543, Loss: 0.7899\n",
      "Epoch 544, Loss: 0.7876\n",
      "Epoch 545, Loss: 0.7853\n",
      "Epoch 546, Loss: 0.7830\n",
      "Epoch 547, Loss: 0.7806\n",
      "Epoch 548, Loss: 0.7783\n",
      "Epoch 549, Loss: 0.7759\n",
      "Epoch 550, Loss: 0.7736\n",
      "Epoch 551, Loss: 0.7712\n",
      "Epoch 552, Loss: 0.7688\n",
      "Epoch 553, Loss: 0.7664\n",
      "Epoch 554, Loss: 0.7640\n",
      "Epoch 555, Loss: 0.7616\n",
      "Epoch 556, Loss: 0.7592\n",
      "Epoch 557, Loss: 0.7568\n",
      "Epoch 558, Loss: 0.7543\n",
      "Epoch 559, Loss: 0.7519\n",
      "Epoch 560, Loss: 0.7494\n",
      "Epoch 561, Loss: 0.7470\n",
      "Epoch 562, Loss: 0.7445\n",
      "Epoch 563, Loss: 0.7420\n",
      "Epoch 564, Loss: 0.7395\n",
      "Epoch 565, Loss: 0.7370\n",
      "Epoch 566, Loss: 0.7345\n",
      "Epoch 567, Loss: 0.7319\n",
      "Epoch 568, Loss: 0.7294\n",
      "Epoch 569, Loss: 0.7268\n",
      "Epoch 570, Loss: 0.7243\n",
      "Epoch 571, Loss: 0.7217\n",
      "Epoch 572, Loss: 0.7191\n",
      "Epoch 573, Loss: 0.7166\n",
      "Epoch 574, Loss: 0.7140\n",
      "Epoch 575, Loss: 0.7114\n",
      "Epoch 576, Loss: 0.7087\n",
      "Epoch 577, Loss: 0.7061\n",
      "Epoch 578, Loss: 0.7035\n",
      "Epoch 579, Loss: 0.7008\n",
      "Epoch 580, Loss: 0.6982\n",
      "Epoch 581, Loss: 0.6955\n",
      "Epoch 582, Loss: 0.6929\n",
      "Epoch 583, Loss: 0.6902\n",
      "Epoch 584, Loss: 0.6875\n",
      "Epoch 585, Loss: 0.6848\n",
      "Epoch 586, Loss: 0.6821\n",
      "Epoch 587, Loss: 0.6794\n",
      "Epoch 588, Loss: 0.6767\n",
      "Epoch 589, Loss: 0.6740\n",
      "Epoch 590, Loss: 0.6713\n",
      "Epoch 591, Loss: 0.6685\n",
      "Epoch 592, Loss: 0.6658\n",
      "Epoch 593, Loss: 0.6630\n",
      "Epoch 594, Loss: 0.6603\n",
      "Epoch 595, Loss: 0.6575\n",
      "Epoch 596, Loss: 0.6548\n",
      "Epoch 597, Loss: 0.6520\n",
      "Epoch 598, Loss: 0.6492\n",
      "Epoch 599, Loss: 0.6465\n",
      "Epoch 600, Loss: 0.6437\n",
      "Epoch 601, Loss: 0.6409\n",
      "Epoch 602, Loss: 0.6381\n",
      "Epoch 603, Loss: 0.6354\n",
      "Epoch 604, Loss: 0.6326\n",
      "Epoch 605, Loss: 0.6298\n",
      "Epoch 606, Loss: 0.6270\n",
      "Epoch 607, Loss: 0.6242\n",
      "Epoch 608, Loss: 0.6214\n",
      "Epoch 609, Loss: 0.6186\n",
      "Epoch 610, Loss: 0.6159\n",
      "Epoch 611, Loss: 0.6131\n",
      "Epoch 612, Loss: 0.6103\n",
      "Epoch 613, Loss: 0.6075\n",
      "Epoch 614, Loss: 0.6047\n",
      "Epoch 615, Loss: 0.6019\n",
      "Epoch 616, Loss: 0.5992\n",
      "Epoch 617, Loss: 0.5964\n",
      "Epoch 618, Loss: 0.5936\n",
      "Epoch 619, Loss: 0.5908\n",
      "Epoch 620, Loss: 0.5881\n",
      "Epoch 621, Loss: 0.5853\n",
      "Epoch 622, Loss: 0.5826\n",
      "Epoch 623, Loss: 0.5798\n",
      "Epoch 624, Loss: 0.5770\n",
      "Epoch 625, Loss: 0.5743\n",
      "Epoch 626, Loss: 0.5716\n",
      "Epoch 627, Loss: 0.5688\n",
      "Epoch 628, Loss: 0.5661\n",
      "Epoch 629, Loss: 0.5634\n",
      "Epoch 630, Loss: 0.5606\n",
      "Epoch 631, Loss: 0.5579\n",
      "Epoch 632, Loss: 0.5552\n",
      "Epoch 633, Loss: 0.5525\n",
      "Epoch 634, Loss: 0.5498\n",
      "Epoch 635, Loss: 0.5471\n",
      "Epoch 636, Loss: 0.5444\n",
      "Epoch 637, Loss: 0.5418\n",
      "Epoch 638, Loss: 0.5391\n",
      "Epoch 639, Loss: 0.5364\n",
      "Epoch 640, Loss: 0.5338\n",
      "Epoch 641, Loss: 0.5311\n",
      "Epoch 642, Loss: 0.5285\n",
      "Epoch 643, Loss: 0.5258\n",
      "Epoch 644, Loss: 0.5232\n",
      "Epoch 645, Loss: 0.5205\n",
      "Epoch 646, Loss: 0.5179\n",
      "Epoch 647, Loss: 0.5153\n",
      "Epoch 648, Loss: 0.5127\n",
      "Epoch 649, Loss: 0.5101\n",
      "Epoch 650, Loss: 0.5075\n",
      "Epoch 651, Loss: 0.5049\n",
      "Epoch 652, Loss: 0.5023\n",
      "Epoch 653, Loss: 0.4998\n",
      "Epoch 654, Loss: 0.4972\n",
      "Epoch 655, Loss: 0.4946\n",
      "Epoch 656, Loss: 0.4921\n",
      "Epoch 657, Loss: 0.4895\n",
      "Epoch 658, Loss: 0.4870\n",
      "Epoch 659, Loss: 0.4844\n",
      "Epoch 660, Loss: 0.4819\n",
      "Epoch 661, Loss: 0.4794\n",
      "Epoch 662, Loss: 0.4769\n",
      "Epoch 663, Loss: 0.4744\n",
      "Epoch 664, Loss: 0.4719\n",
      "Epoch 665, Loss: 0.4694\n",
      "Epoch 666, Loss: 0.4669\n",
      "Epoch 667, Loss: 0.4644\n",
      "Epoch 668, Loss: 0.4619\n",
      "Epoch 669, Loss: 0.4595\n",
      "Epoch 670, Loss: 0.4570\n",
      "Epoch 671, Loss: 0.4546\n",
      "Epoch 672, Loss: 0.4521\n",
      "Epoch 673, Loss: 0.4497\n",
      "Epoch 674, Loss: 0.4472\n",
      "Epoch 675, Loss: 0.4448\n",
      "Epoch 676, Loss: 0.4424\n",
      "Epoch 677, Loss: 0.4400\n",
      "Epoch 678, Loss: 0.4376\n",
      "Epoch 679, Loss: 0.4352\n",
      "Epoch 680, Loss: 0.4328\n",
      "Epoch 681, Loss: 0.4304\n",
      "Epoch 682, Loss: 0.4280\n",
      "Epoch 683, Loss: 0.4257\n",
      "Epoch 684, Loss: 0.4233\n",
      "Epoch 685, Loss: 0.4209\n",
      "Epoch 686, Loss: 0.4186\n",
      "Epoch 687, Loss: 0.4162\n",
      "Epoch 688, Loss: 0.4139\n",
      "Epoch 689, Loss: 0.4116\n",
      "Epoch 690, Loss: 0.4093\n",
      "Epoch 691, Loss: 0.4069\n",
      "Epoch 692, Loss: 0.4046\n",
      "Epoch 693, Loss: 0.4023\n",
      "Epoch 694, Loss: 0.4001\n",
      "Epoch 695, Loss: 0.3978\n",
      "Epoch 696, Loss: 0.3955\n",
      "Epoch 697, Loss: 0.3932\n",
      "Epoch 698, Loss: 0.3910\n",
      "Epoch 699, Loss: 0.3887\n",
      "Epoch 700, Loss: 0.3865\n",
      "Epoch 701, Loss: 0.3842\n",
      "Epoch 702, Loss: 0.3820\n",
      "Epoch 703, Loss: 0.3798\n",
      "Epoch 704, Loss: 0.3775\n",
      "Epoch 705, Loss: 0.3753\n",
      "Epoch 706, Loss: 0.3731\n",
      "Epoch 707, Loss: 0.3709\n",
      "Epoch 708, Loss: 0.3687\n",
      "Epoch 709, Loss: 0.3666\n",
      "Epoch 710, Loss: 0.3644\n",
      "Epoch 711, Loss: 0.3622\n",
      "Epoch 712, Loss: 0.3601\n",
      "Epoch 713, Loss: 0.3579\n",
      "Epoch 714, Loss: 0.3558\n",
      "Epoch 715, Loss: 0.3537\n",
      "Epoch 716, Loss: 0.3515\n",
      "Epoch 717, Loss: 0.3494\n",
      "Epoch 718, Loss: 0.3473\n",
      "Epoch 719, Loss: 0.3452\n",
      "Epoch 720, Loss: 0.3431\n",
      "Epoch 721, Loss: 0.3410\n",
      "Epoch 722, Loss: 0.3389\n",
      "Epoch 723, Loss: 0.3369\n",
      "Epoch 724, Loss: 0.3348\n",
      "Epoch 725, Loss: 0.3328\n",
      "Epoch 726, Loss: 0.3307\n",
      "Epoch 727, Loss: 0.3287\n",
      "Epoch 728, Loss: 0.3267\n",
      "Epoch 729, Loss: 0.3246\n",
      "Epoch 730, Loss: 0.3226\n",
      "Epoch 731, Loss: 0.3206\n",
      "Epoch 732, Loss: 0.3186\n",
      "Epoch 733, Loss: 0.3166\n",
      "Epoch 734, Loss: 0.3147\n",
      "Epoch 735, Loss: 0.3127\n",
      "Epoch 736, Loss: 0.3107\n",
      "Epoch 737, Loss: 0.3088\n",
      "Epoch 738, Loss: 0.3068\n",
      "Epoch 739, Loss: 0.3049\n",
      "Epoch 740, Loss: 0.3030\n",
      "Epoch 741, Loss: 0.3010\n",
      "Epoch 742, Loss: 0.2991\n",
      "Epoch 743, Loss: 0.2972\n",
      "Epoch 744, Loss: 0.2953\n",
      "Epoch 745, Loss: 0.2934\n",
      "Epoch 746, Loss: 0.2916\n",
      "Epoch 747, Loss: 0.2897\n",
      "Epoch 748, Loss: 0.2878\n",
      "Epoch 749, Loss: 0.2860\n",
      "Epoch 750, Loss: 0.2841\n",
      "Epoch 751, Loss: 0.2823\n",
      "Epoch 752, Loss: 0.2805\n",
      "Epoch 753, Loss: 0.2787\n",
      "Epoch 754, Loss: 0.2768\n",
      "Epoch 755, Loss: 0.2750\n",
      "Epoch 756, Loss: 0.2733\n",
      "Epoch 757, Loss: 0.2715\n",
      "Epoch 758, Loss: 0.2697\n",
      "Epoch 759, Loss: 0.2679\n",
      "Epoch 760, Loss: 0.2662\n",
      "Epoch 761, Loss: 0.2644\n",
      "Epoch 762, Loss: 0.2627\n",
      "Epoch 763, Loss: 0.2610\n",
      "Epoch 764, Loss: 0.2592\n",
      "Epoch 765, Loss: 0.2575\n",
      "Epoch 766, Loss: 0.2558\n",
      "Epoch 767, Loss: 0.2541\n",
      "Epoch 768, Loss: 0.2524\n",
      "Epoch 769, Loss: 0.2508\n",
      "Epoch 770, Loss: 0.2491\n",
      "Epoch 771, Loss: 0.2474\n",
      "Epoch 772, Loss: 0.2458\n",
      "Epoch 773, Loss: 0.2441\n",
      "Epoch 774, Loss: 0.2425\n",
      "Epoch 775, Loss: 0.2409\n",
      "Epoch 776, Loss: 0.2393\n",
      "Epoch 777, Loss: 0.2376\n",
      "Epoch 778, Loss: 0.2360\n",
      "Epoch 779, Loss: 0.2345\n",
      "Epoch 780, Loss: 0.2329\n",
      "Epoch 781, Loss: 0.2313\n",
      "Epoch 782, Loss: 0.2297\n",
      "Epoch 783, Loss: 0.2282\n",
      "Epoch 784, Loss: 0.2266\n",
      "Epoch 785, Loss: 0.2251\n",
      "Epoch 786, Loss: 0.2236\n",
      "Epoch 787, Loss: 0.2220\n",
      "Epoch 788, Loss: 0.2205\n",
      "Epoch 789, Loss: 0.2190\n",
      "Epoch 790, Loss: 0.2175\n",
      "Epoch 791, Loss: 0.2161\n",
      "Epoch 792, Loss: 0.2146\n",
      "Epoch 793, Loss: 0.2131\n",
      "Epoch 794, Loss: 0.2116\n",
      "Epoch 795, Loss: 0.2102\n",
      "Epoch 796, Loss: 0.2088\n",
      "Epoch 797, Loss: 0.2073\n",
      "Epoch 798, Loss: 0.2059\n",
      "Epoch 799, Loss: 0.2045\n",
      "Epoch 800, Loss: 0.2031\n",
      "Epoch 801, Loss: 0.2017\n",
      "Epoch 802, Loss: 0.2003\n",
      "Epoch 803, Loss: 0.1989\n",
      "Epoch 804, Loss: 0.1975\n",
      "Epoch 805, Loss: 0.1962\n",
      "Epoch 806, Loss: 0.1948\n",
      "Epoch 807, Loss: 0.1934\n",
      "Epoch 808, Loss: 0.1921\n",
      "Epoch 809, Loss: 0.1908\n",
      "Epoch 810, Loss: 0.1894\n",
      "Epoch 811, Loss: 0.1881\n",
      "Epoch 812, Loss: 0.1868\n",
      "Epoch 813, Loss: 0.1855\n",
      "Epoch 814, Loss: 0.1842\n",
      "Epoch 815, Loss: 0.1829\n",
      "Epoch 816, Loss: 0.1817\n",
      "Epoch 817, Loss: 0.1804\n",
      "Epoch 818, Loss: 0.1792\n",
      "Epoch 819, Loss: 0.1779\n",
      "Epoch 820, Loss: 0.1767\n",
      "Epoch 821, Loss: 0.1754\n",
      "Epoch 822, Loss: 0.1742\n",
      "Epoch 823, Loss: 0.1730\n",
      "Epoch 824, Loss: 0.1718\n",
      "Epoch 825, Loss: 0.1706\n",
      "Epoch 826, Loss: 0.1694\n",
      "Epoch 827, Loss: 0.1682\n",
      "Epoch 828, Loss: 0.1670\n",
      "Epoch 829, Loss: 0.1658\n",
      "Epoch 830, Loss: 0.1647\n",
      "Epoch 831, Loss: 0.1635\n",
      "Epoch 832, Loss: 0.1624\n",
      "Epoch 833, Loss: 0.1612\n",
      "Epoch 834, Loss: 0.1601\n",
      "Epoch 835, Loss: 0.1590\n",
      "Epoch 836, Loss: 0.1579\n",
      "Epoch 837, Loss: 0.1568\n",
      "Epoch 838, Loss: 0.1557\n",
      "Epoch 839, Loss: 0.1546\n",
      "Epoch 840, Loss: 0.1535\n",
      "Epoch 841, Loss: 0.1524\n",
      "Epoch 842, Loss: 0.1514\n",
      "Epoch 843, Loss: 0.1503\n",
      "Epoch 844, Loss: 0.1492\n",
      "Epoch 845, Loss: 0.1482\n",
      "Epoch 846, Loss: 0.1472\n",
      "Epoch 847, Loss: 0.1461\n",
      "Epoch 848, Loss: 0.1451\n",
      "Epoch 849, Loss: 0.1441\n",
      "Epoch 850, Loss: 0.1431\n",
      "Epoch 851, Loss: 0.1421\n",
      "Epoch 852, Loss: 0.1411\n",
      "Epoch 853, Loss: 0.1401\n",
      "Epoch 854, Loss: 0.1391\n",
      "Epoch 855, Loss: 0.1381\n",
      "Epoch 856, Loss: 0.1372\n",
      "Epoch 857, Loss: 0.1362\n",
      "Epoch 858, Loss: 0.1352\n",
      "Epoch 859, Loss: 0.1343\n",
      "Epoch 860, Loss: 0.1334\n",
      "Epoch 861, Loss: 0.1324\n",
      "Epoch 862, Loss: 0.1315\n",
      "Epoch 863, Loss: 0.1306\n",
      "Epoch 864, Loss: 0.1297\n",
      "Epoch 865, Loss: 0.1288\n",
      "Epoch 866, Loss: 0.1279\n",
      "Epoch 867, Loss: 0.1270\n",
      "Epoch 868, Loss: 0.1261\n",
      "Epoch 869, Loss: 0.1252\n",
      "Epoch 870, Loss: 0.1244\n",
      "Epoch 871, Loss: 0.1235\n",
      "Epoch 872, Loss: 0.1226\n",
      "Epoch 873, Loss: 0.1218\n",
      "Epoch 874, Loss: 0.1209\n",
      "Epoch 875, Loss: 0.1201\n",
      "Epoch 876, Loss: 0.1193\n",
      "Epoch 877, Loss: 0.1184\n",
      "Epoch 878, Loss: 0.1176\n",
      "Epoch 879, Loss: 0.1168\n",
      "Epoch 880, Loss: 0.1160\n",
      "Epoch 881, Loss: 0.1152\n",
      "Epoch 882, Loss: 0.1144\n",
      "Epoch 883, Loss: 0.1136\n",
      "Epoch 884, Loss: 0.1128\n",
      "Epoch 885, Loss: 0.1121\n",
      "Epoch 886, Loss: 0.1113\n",
      "Epoch 887, Loss: 0.1105\n",
      "Epoch 888, Loss: 0.1098\n",
      "Epoch 889, Loss: 0.1090\n",
      "Epoch 890, Loss: 0.1083\n",
      "Epoch 891, Loss: 0.1075\n",
      "Epoch 892, Loss: 0.1068\n",
      "Epoch 893, Loss: 0.1061\n",
      "Epoch 894, Loss: 0.1053\n",
      "Epoch 895, Loss: 0.1046\n",
      "Epoch 896, Loss: 0.1039\n",
      "Epoch 897, Loss: 0.1032\n",
      "Epoch 898, Loss: 0.1025\n",
      "Epoch 899, Loss: 0.1018\n",
      "Epoch 900, Loss: 0.1011\n",
      "Epoch 901, Loss: 0.1004\n",
      "Epoch 902, Loss: 0.0998\n",
      "Epoch 903, Loss: 0.0991\n",
      "Epoch 904, Loss: 0.0984\n",
      "Epoch 905, Loss: 0.0978\n",
      "Epoch 906, Loss: 0.0971\n",
      "Epoch 907, Loss: 0.0965\n",
      "Epoch 908, Loss: 0.0958\n",
      "Epoch 909, Loss: 0.0952\n",
      "Epoch 910, Loss: 0.0945\n",
      "Epoch 911, Loss: 0.0939\n",
      "Epoch 912, Loss: 0.0933\n",
      "Epoch 913, Loss: 0.0927\n",
      "Epoch 914, Loss: 0.0921\n",
      "Epoch 915, Loss: 0.0914\n",
      "Epoch 916, Loss: 0.0908\n",
      "Epoch 917, Loss: 0.0902\n",
      "Epoch 918, Loss: 0.0896\n",
      "Epoch 919, Loss: 0.0891\n",
      "Epoch 920, Loss: 0.0885\n",
      "Epoch 921, Loss: 0.0879\n",
      "Epoch 922, Loss: 0.0873\n",
      "Epoch 923, Loss: 0.0867\n",
      "Epoch 924, Loss: 0.0862\n",
      "Epoch 925, Loss: 0.0856\n",
      "Epoch 926, Loss: 0.0851\n",
      "Epoch 927, Loss: 0.0845\n",
      "Epoch 928, Loss: 0.0840\n",
      "Epoch 929, Loss: 0.0834\n",
      "Epoch 930, Loss: 0.0829\n",
      "Epoch 931, Loss: 0.0824\n",
      "Epoch 932, Loss: 0.0818\n",
      "Epoch 933, Loss: 0.0813\n",
      "Epoch 934, Loss: 0.0808\n",
      "Epoch 935, Loss: 0.0803\n",
      "Epoch 936, Loss: 0.0798\n",
      "Epoch 937, Loss: 0.0792\n",
      "Epoch 938, Loss: 0.0787\n",
      "Epoch 939, Loss: 0.0782\n",
      "Epoch 940, Loss: 0.0778\n",
      "Epoch 941, Loss: 0.0773\n",
      "Epoch 942, Loss: 0.0768\n",
      "Epoch 943, Loss: 0.0763\n",
      "Epoch 944, Loss: 0.0758\n",
      "Epoch 945, Loss: 0.0753\n",
      "Epoch 946, Loss: 0.0749\n",
      "Epoch 947, Loss: 0.0744\n",
      "Epoch 948, Loss: 0.0739\n",
      "Epoch 949, Loss: 0.0735\n",
      "Epoch 950, Loss: 0.0730\n",
      "Epoch 951, Loss: 0.0726\n",
      "Epoch 952, Loss: 0.0721\n",
      "Epoch 953, Loss: 0.0717\n",
      "Epoch 954, Loss: 0.0713\n",
      "Epoch 955, Loss: 0.0708\n",
      "Epoch 956, Loss: 0.0704\n",
      "Epoch 957, Loss: 0.0700\n",
      "Epoch 958, Loss: 0.0695\n",
      "Epoch 959, Loss: 0.0691\n",
      "Epoch 960, Loss: 0.0687\n",
      "Epoch 961, Loss: 0.0683\n",
      "Epoch 962, Loss: 0.0679\n",
      "Epoch 963, Loss: 0.0675\n",
      "Epoch 964, Loss: 0.0671\n",
      "Epoch 965, Loss: 0.0667\n",
      "Epoch 966, Loss: 0.0663\n",
      "Epoch 967, Loss: 0.0659\n",
      "Epoch 968, Loss: 0.0655\n",
      "Epoch 969, Loss: 0.0651\n",
      "Epoch 970, Loss: 0.0647\n",
      "Epoch 971, Loss: 0.0644\n",
      "Epoch 972, Loss: 0.0640\n",
      "Epoch 973, Loss: 0.0636\n",
      "Epoch 974, Loss: 0.0633\n",
      "Epoch 975, Loss: 0.0629\n",
      "Epoch 976, Loss: 0.0625\n",
      "Epoch 977, Loss: 0.0622\n",
      "Epoch 978, Loss: 0.0618\n",
      "Epoch 979, Loss: 0.0615\n",
      "Epoch 980, Loss: 0.0611\n",
      "Epoch 981, Loss: 0.0608\n",
      "Epoch 982, Loss: 0.0604\n",
      "Epoch 983, Loss: 0.0601\n",
      "Epoch 984, Loss: 0.0598\n",
      "Epoch 985, Loss: 0.0594\n",
      "Epoch 986, Loss: 0.0591\n",
      "Epoch 987, Loss: 0.0588\n",
      "Epoch 988, Loss: 0.0584\n",
      "Epoch 989, Loss: 0.0581\n",
      "Epoch 990, Loss: 0.0578\n",
      "Epoch 991, Loss: 0.0575\n",
      "Epoch 992, Loss: 0.0572\n",
      "Epoch 993, Loss: 0.0569\n",
      "Epoch 994, Loss: 0.0565\n",
      "Epoch 995, Loss: 0.0562\n",
      "Epoch 996, Loss: 0.0559\n",
      "Epoch 997, Loss: 0.0556\n",
      "Epoch 998, Loss: 0.0553\n",
      "Epoch 999, Loss: 0.0550\n"
     ]
    }
   ],
   "source": [
    "# ---------- Run ----------\n",
    "X = df[['cgpa', 'profile_score']].values.T  # shape (features, samples)\n",
    "Y = df[['lpa']].values.T\n",
    "\n",
    "layer_dims = [2, 3, 2, 1]  # 2-input  3-hidden  2-hidden  1-output\n",
    "params,loss = train_model(X, Y, layer_dims, learning_rate=0.01, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d85a1df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2433022133387805)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6794bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d80eb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2433022133387805)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ef8f5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13a4357b0b0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801fad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
