{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27743a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adhab\\OneDrive\\Desktop\\VsCode\\DataScience\\Deep Learning\\deeplen\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "\n",
    "# Build a simple model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f9a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c358dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step - accuracy: 0.0769 - loss: 2.3248\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.0804 - loss: 2.3153\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.0844 - loss: 2.3062\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.0879 - loss: 2.2975\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.0917 - loss: 2.2890\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.0957 - loss: 2.2808\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.0998 - loss: 2.2729\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.1037 - loss: 2.2652\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.1084 - loss: 2.2577\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.1129 - loss: 2.2503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x201fc8f3590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch Gradient Descent â†’ batch_size = full dataset\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b417cdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9742 - loss: 0.0820\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9747 - loss: 0.0802\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9751 - loss: 0.0787\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9756 - loss: 0.0773\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9759 - loss: 0.0761\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9763 - loss: 0.0750\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9766 - loss: 0.0740\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9769 - loss: 0.0731\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9772 - loss: 0.0723\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9775 - loss: 0.0715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2018605fa10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mini-Batch Gradient Descent (Most common)\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182f6c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60000/60000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2ms/step - accuracy: 0.9597 - loss: 0.1315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20186052cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf6af8",
   "metadata": {},
   "source": [
    "## ğŸ§  Types of Gradient Descent in Neural Networks\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the loss function in neural networks by updating the model's weights iteratively.  \n",
    "Depending on how much data is used to compute the gradient at each iteration, there are three main types:\n",
    "\n",
    "| **Type** | **Description** | **Pros** | **Cons** |\n",
    "|-----------|-----------------|-----------|-----------|\n",
    "| **Batch Gradient Descent** | Uses **all training samples** to compute the gradient before each parameter update. The entire dataset is processed to calculate a single update step. | â€¢ Produces **stable and smooth convergence** since it uses the complete dataset.<br>â€¢ Converges to the **global minimum** for convex error surfaces and a good local minimum for non-convex ones.<br>â€¢ Deterministic behavior (same updates for same data). | â€¢ **Computationally expensive** for large datasets.<br>â€¢ **High memory usage**, since all data must fit into memory.<br>â€¢ **Slow updates**, making training time very long. |\n",
    "| **Stochastic Gradient Descent (SGD)** | Updates weights **after processing each individual training example**. Each sample causes a parameter update. | â€¢ **Faster updates** â€“ can start learning before seeing the full dataset.<br>â€¢ Helps **escape local minima** due to noisy gradient updates.<br>â€¢ Works well for **online learning** or streaming data. | â€¢ **Highly noisy convergence** â€“ may fluctuate around the minimum.<br>â€¢ **Harder to tune** learning rate.<br>â€¢ May **never truly converge**, only oscillate near the minimum. |\n",
    "| **Mini-Batch Gradient Descent** | Uses a **subset (batch)** of the training data (e.g., 32, 64, 128 samples) to compute the gradient and update weights. Itâ€™s a compromise between batch and stochastic methods. | â€¢ **Balances efficiency and stability** â€“ faster than batch, more stable than SGD.<br>â€¢ **Vectorized operations** (GPU/TPU friendly).<br>â€¢ **Smoother convergence** than pure SGD.<br>â€¢ Allows use of **regularization techniques** like Batch Normalization. | â€¢ Requires **tuning batch size** â€“ too small adds noise, too large slows learning.<br>â€¢ **May get stuck** in shallow local minima.<br>â€¢ Performance depends on **hardware memory** limits. |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Summary\n",
    "\n",
    "- **Batch Gradient Descent** â†’ Stable but slow.  \n",
    "- **Stochastic Gradient Descent (SGD)** â†’ Fast but noisy.  \n",
    "- **Mini-Batch Gradient Descent** â†’ The best of both worlds; commonly used in Deep Learning.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ In Keras\n",
    "\n",
    "You can control which type to use by changing the `batch_size` parameter in `model.fit()`:\n",
    "\n",
    "```python\n",
    "# Batch Gradient Descent\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=len(x_train))\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "# Mini-Batch Gradient Descent (default)\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
